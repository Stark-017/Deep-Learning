{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4413c5b-0271-471c-8ec2-3adbf098fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pneumonia detection — 4-model comparison + custom RNN-style model + bounding-box via Grad-CAM\n",
    "Save this as `pneumonia_detection_multimodel.py` and run with Python (ideally in a GPU environment).\n",
    "\n",
    "Assumptions about dataset layout (as you described):\n",
    "dataset/\n",
    "  train/\n",
    "    NORMAL/\n",
    "    PNEUMONIA/\n",
    "       virus/\n",
    "       bacteria/\n",
    "  test/\n",
    "    NORMAL/\n",
    "    PNEUMONIA/\n",
    "\n",
    "Notes:\n",
    "- This script uses TensorFlow / Keras.\n",
    "- Models: (1) Pretrained EfficientNetB0 (transfer learning)\n",
    "          (2) Simple custom CNN\n",
    "          (3) CNN -> BiLSTM (treat image as sequence of row-features)\n",
    "          (4) A \"custom RNN\" that uses TimeDistributed Conv on patches + LSTM\n",
    "- Bounding box extraction uses Grad-CAM heatmap and finds the largest bright region bounding box.\n",
    "- Trains each model for `EPOCHS` (default 100). You can reduce for testing.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff008b-a9b4-48e0-ab70-390d770bbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:/Users/aravr/Downloads/Pneumonia Detection Dataset/chest_xray\"  # change to your path\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 100  # per your request\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 42\n",
    "NUM_CLASSES = 2  # normal vs pneumonia\n",
    "\n",
    "# -------- helper: load datasets --------\n",
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# cache & prefetch for speed\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# normalization layer\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# -------- metrics & compile helper --------\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "def compile_and_train(model, name, epochs=EPOCHS):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    history = model.fit(\n",
    "        train_ds.map(lambda x,y: (normalization_layer(x), y)),\n",
    "        validation_data=val_ds.map(lambda x,y: (normalization_layer(x), y)),\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.save(f\"{name}.h5\")\n",
    "    return history\n",
    "\n",
    "# -------- Model 1: Transfer learning EfficientNetB0 --------\n",
    "def build_efficientnet_model(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    base = EfficientNetB0(include_top=False, input_shape=(*img_size,3), weights='imagenet')\n",
    "    base.trainable = False\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=out)\n",
    "    return model\n",
    "\n",
    "# -------- Model 2: Simple custom CNN --------\n",
    "def build_simple_cnn(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Model 3: CNN features -> BiLSTM (treat rows as sequence) --------\n",
    "# Idea: apply small conv to get H x W x C features, then treat H rows as sequence where each row's\n",
    "# features are pooled across width -> sequence length = H, then BiLSTM\n",
    "\n",
    "def build_cnn_bilstm(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    x = layers.Conv2D(32,3,activation='relu',padding='same')(inputs)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    # feature map shape: (H', W', C')\n",
    "    # we will pool across width -> sequence of length H'\n",
    "    f = layers.Permute((2,1,3))(x)  # swap axes so we can TimeDistributed pool across width\n",
    "    # collapse width into channels using GlobalMaxPool across width inside TimeDistributed\n",
    "    seq = layers.TimeDistributed(layers.GlobalMaxPooling1D())(layers.Reshape((-1, x.shape[2]*x.shape[3]))(f))\n",
    "    # Above reshape may be dynamic; simpler approach below:\n",
    "    # alternative: reduce across width dimension with Lambda\n",
    "    seq = layers.Lambda(lambda z: tf.reduce_max(z, axis=2))(x)  # now (batch, H', C')\n",
    "    # feed to BiLSTM\n",
    "    seq = layers.Bidirectional(layers.LSTM(64))(seq)\n",
    "    x = layers.Dense(64, activation='relu')(seq)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Model 4: Custom RNN-style: patches -> TimeDistributed Conv -> LSTM --------\n",
    "# We split image into vertical patches (like columns) and treat each column as timestep\n",
    "\n",
    "def build_custom_rnn(img_size=IMG_SIZE, num_classes=NUM_CLASSES, num_patches=28):\n",
    "    # num_patches decides sequence length (e.g., 28 columns -> 28 timesteps)\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    # resize to ensure divisible\n",
    "    h, w = img_size\n",
    "    patch_w = w // num_patches\n",
    "    # extract patches using Reshape after resizing to (h, num_patches, patch_w, 3)\n",
    "    x = layers.Resizing(h, num_patches*patch_w)(inputs)\n",
    "    x = layers.Reshape((h, num_patches, patch_w, 3))(x)\n",
    "    # process each patch with small conv net\n",
    "    td = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(x)\n",
    "    td = layers.TimeDistributed(layers.MaxPool2D())(td)\n",
    "    td = layers.TimeDistributed(layers.Flatten())(td)\n",
    "    # now sequence shape: (batch, num_patches, features)\n",
    "    r = layers.LSTM(128)(td)\n",
    "    x = layers.Dense(64, activation='relu')(r)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Grad-CAM + bounding box helper --------\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # img_array: preprocessed single image (1, H, W, 3)\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-9)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def heatmap_to_bbox(heatmap, orig_img_shape, upsample_size=IMG_SIZE):\n",
    "    # Resize heatmap to image size\n",
    "    hm = cv2.resize(heatmap, upsample_size)\n",
    "    hm_uint = np.uint8(255 * hm)\n",
    "    # threshold and find contours\n",
    "    _, th = cv2.threshold(hm_uint, 100, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    # choose largest contour\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    return (x,y,w,h)\n",
    "\n",
    "# visualization helper\n",
    "\n",
    "def display_bbox_on_image(img, bbox, title=None):\n",
    "    img = img.copy()\n",
    "    if bbox is not None:\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    if title: plt.title(title)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# -------- build models --------\n",
    "models = {}\n",
    "models['efficientnet'] = build_efficientnet_model()\n",
    "models['simple_cnn'] = build_simple_cnn()\n",
    "models['cnn_bilstm'] = build_cnn_bilstm()\n",
    "models['custom_rnn'] = build_custom_rnn()\n",
    "\n",
    "# -------- Train models (WARNING: this will take time) --------\n",
    "histories = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        histories[name] = compile_and_train(model, name, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed for {name}: {e}\")\n",
    "\n",
    "# -------- Example: run Grad-CAM on one test image using EfficientNet (if available) --------\n",
    "# We need to pick a model with a convolutional last layer. EfficientNet has 'top_conv' as last conv.\n",
    "if os.path.exists('efficientnet.h5'):\n",
    "    model = keras.models.load_model('efficientnet.h5')\n",
    "    # pick an image from val_ds\n",
    "    for images, labels in val_ds.take(1):\n",
    "        img = images[0].numpy()\n",
    "        lbl = labels[0].numpy()\n",
    "        break\n",
    "    img_input = np.expand_dims(img/255.0, axis=0)\n",
    "    # ensure we can access last conv layer name\n",
    "    last_conv = None\n",
    "    for layer in model.layers[::-1]:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            last_conv = layer.name\n",
    "            break\n",
    "    if last_conv:\n",
    "        heatmap = make_gradcam_heatmap(img_input, model, last_conv)\n",
    "        bbox = heatmap_to_bbox(heatmap, img.shape[:2], upsample_size=(IMG_SIZE[1], IMG_SIZE[0]))\n",
    "        display_bbox_on_image((img).astype('uint8'), bbox, title=f\"label={lbl}\")\n",
    "    else:\n",
    "        print(\"No Conv2D layer found for Grad-CAM in loaded model.\")\n",
    "\n",
    "# -------- Plot accuracy comparison --------\n",
    "plt.figure(figsize=(10,6))\n",
    "for name, h in histories.items():\n",
    "    acc = h.history.get('accuracy') or h.history.get('acc')\n",
    "    val_acc = h.history.get('val_accuracy') or h.history.get('val_acc')\n",
    "    epochs_range = range(1, len(acc)+1)\n",
    "    plt.plot(epochs_range, acc, label=f\"{name} train\")\n",
    "    plt.plot(epochs_range, val_acc, '--', label=f\"{name} val\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model comparison')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDone. Models saved as <name>.h5. Use the saved models to run inference or Grad-CAM later.\")\n",
    "\n",
    "# -------- Suggestions for improving & unique ideas (printed for clarity) --------\n",
    "improvements = [\n",
    "    \"1) Add segmentation: train a U-Net to segment infected regions instead of only bounding boxes.\",\n",
    "    \"2) Use multiclass classification to detect virus vs bacteria vs normal (your dataset contains those subfolders).\",\n",
    "    \"3) Use explainability: besides Grad-CAM use integrated gradients and shap for trustworthy explanations.\",\n",
    "    \"4) Use data augmentation tuned for X-rays: elastic deformations, contrast adjustments, random rotations +/- 10 degrees.\",\n",
    "    \"5) Cross-validation and reporting sensitivity, specificity, F1-score, ROC AUC (medical tasks care about sensitivity).\",\n",
    "    \"6) Build an ensemble of top-2 models to boost accuracy and robustness.\",\n",
    "    \"7) Add patient metadata (if available) like age/gender and create multimodal model combining image + metadata.\",\n",
    "    \"8) Create a lightweight model (pruning/quantization) and measure inference latency to deploy on edge devices.\",\n",
    "    \"9) Create a small web app (Streamlit / Flask) to upload an X-ray and show prediction + bounding box + explanation.\",\n",
    "]\n",
    "\n",
    "print('\\nSuggestions to make project more valuable:')\n",
    "for s in improvements:\n",
    "    print(s)\n",
    "\n",
    "# End of script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436ae33-0fb0-441f-9fe5-2d3380d511ac",
   "metadata": {},
   "source": [
    "### \"\"\"\n",
    "Pneumonia detection — 4-model comparison + custom RNN-style model + bounding-box via Grad-CAM\n",
    "Dataset structure (as per your screenshots):\n",
    "\n",
    "Pneumonia Detection Dataset/chest_xray/\n",
    "  train/\n",
    "    NORMAL/\n",
    "    PNEUMONIA/\n",
    "  test/\n",
    "    NORMAL/\n",
    "    PNEUMONIA/\n",
    "\n",
    "Notes:\n",
    "- This script uses TensorFlow / Keras.\n",
    "- Models: (1) Pretrained EfficientNetB0 (transfer learning)\n",
    "          (2) Simple custom CNN\n",
    "          (3) CNN -> BiLSTM (treat image as sequence of row-features)\n",
    "          (4) A \"custom RNN\" that uses TimeDistributed Conv on patches + LSTM\n",
    "- Bounding box extraction uses Grad-CAM heatmap and finds the largest bright region bounding box.\n",
    "- Trains each model for `EPOCHS` (default 100).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2\n",
    "\n",
    "# -------- user params --------\n",
    "DATA_DIR = r\"C:/Users/aravr/Downloads/Pneumonia Detection Dataset/chest_xray\"  # adjust to your path\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 7\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SEED = 42\n",
    "NUM_CLASSES = 2  # normal vs pneumonia\n",
    "\n",
    "# -------- helper: load datasets --------\n",
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# cache & prefetch for speed\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# normalization layer\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# -------- metrics & compile helper --------\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "def compile_and_train(model, name, epochs=EPOCHS):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    history = model.fit(\n",
    "        train_ds.map(lambda x,y: (normalization_layer(x), y)),\n",
    "        validation_data=val_ds.map(lambda x,y: (normalization_layer(x), y)),\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.save(f\"{name}.h5\")\n",
    "    return history\n",
    "\n",
    "# -------- Model 1: Transfer learning EfficientNetB0 --------\n",
    "def build_efficientnet_model(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    base = EfficientNetB0(include_top=False, input_shape=(*img_size,3), weights='imagenet')\n",
    "    base.trainable = False\n",
    "    x = layers.GlobalAveragePooling2D()(base.output)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base.input, outputs=out)\n",
    "    return model\n",
    "\n",
    "# -------- Model 2: Simple custom CNN --------\n",
    "def build_simple_cnn(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Model 3: CNN features -> BiLSTM --------\n",
    "def build_cnn_bilstm(img_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    x = layers.Conv2D(32,3,activation='relu',padding='same')(inputs)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    seq = layers.Lambda(lambda z: tf.reduce_max(z, axis=2))(x)  # (batch, H', C')\n",
    "    seq = layers.Bidirectional(layers.LSTM(64))(seq)\n",
    "    x = layers.Dense(64, activation='relu')(seq)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Model 4: Custom RNN-style --------\n",
    "def build_custom_rnn(img_size=IMG_SIZE, num_classes=NUM_CLASSES, num_patches=28):\n",
    "    inputs = keras.Input(shape=(*img_size,3))\n",
    "    h, w = img_size\n",
    "    patch_w = w // num_patches\n",
    "    x = layers.Resizing(h, num_patches*patch_w)(inputs)\n",
    "    x = layers.Reshape((h, num_patches, patch_w, 3))(x)\n",
    "    td = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(x)\n",
    "    td = layers.TimeDistributed(layers.MaxPool2D())(td)\n",
    "    td = layers.TimeDistributed(layers.Flatten())(td)\n",
    "    r = layers.LSTM(128)(td)\n",
    "    x = layers.Dense(64, activation='relu')(r)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs, out)\n",
    "\n",
    "# -------- Grad-CAM + bounding box --------\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-9)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def heatmap_to_bbox(heatmap, upsample_size=IMG_SIZE):\n",
    "    hm = cv2.resize(heatmap, upsample_size)\n",
    "    hm_uint = np.uint8(255 * hm)\n",
    "    _, th = cv2.threshold(hm_uint, 100, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    return (x,y,w,h)\n",
    "\n",
    "def display_bbox_on_image(img, bbox, title=None):\n",
    "    img = img.copy()\n",
    "    if bbox is not None:\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    if title: plt.title(title)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# -------- build models --------\n",
    "models = {\n",
    "    'efficientnet': build_efficientnet_model(),\n",
    "    'simple_cnn': build_simple_cnn(),\n",
    "    'cnn_bilstm': build_cnn_bilstm(),\n",
    "    'custom_rnn': build_custom_rnn()\n",
    "}\n",
    "\n",
    "# -------- Train models --------\n",
    "histories = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        histories[name] = compile_and_train(model, name, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed for {name}: {e}\")\n",
    "\n",
    "# -------- Example Grad-CAM on EfficientNet --------\n",
    "if os.path.exists('efficientnet.h5'):\n",
    "    model = keras.models.load_model('efficientnet.h5')\n",
    "    for images, labels in val_ds.take(1):\n",
    "        img = images[0].numpy()\n",
    "        lbl = labels[0].numpy()\n",
    "        break\n",
    "    img_input = np.expand_dims(img/255.0, axis=0)\n",
    "    last_conv = None\n",
    "    for layer in model.layers[::-1]:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            last_conv = layer.name\n",
    "            break\n",
    "    if last_conv:\n",
    "        heatmap = make_gradcam_heatmap(img_input, model, last_conv)\n",
    "        bbox = heatmap_to_bbox(heatmap, upsample_size=(IMG_SIZE[1], IMG_SIZE[0]))\n",
    "        display_bbox_on_image((img).astype('uint8'), bbox, title=f\"label={lbl}\")\n",
    "\n",
    "# -------- Plot accuracy comparison --------\n",
    "plt.figure(figsize=(10,6))\n",
    "for name, h in histories.items():\n",
    "    acc = h.history.get('accuracy') or h.history.get('acc')\n",
    "    val_acc = h.history.get('val_accuracy') or h.history.get('val_acc')\n",
    "    epochs_range = range(1, len(acc)+1)\n",
    "    plt.plot(epochs_range, acc, label=f\"{name} train\")\n",
    "    plt.plot(epochs_range, val_acc, '--', label=f\"{name} val\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model comparison')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDone. Models saved as <name>.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbf251-b9a1-4900-b47c-53fa559014e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
